{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0096941-62f3-4e24-8dca-3984822198ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "import importlib.util\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c48ac727-fc51-4cec-b878-6b98d2f2d6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/vmuser/Assignments/problem.ipynb\"\n",
    "print(\"File exists:\", os.path.exists(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa948914-3589-446b-9a21-2341b351bc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIChatbot    Pictures\t     evaluation\n",
      "Assignments  Public\t     google-chrome-stable_current_amd64.deb\n",
      "Desktop      Templates\t     problem.ipynb\n",
      "Documents    Videos\t     task1\n",
      "Downloads    auto\t     testprojects\n",
      "Music\t     evaluate.ipynb  thinclient_drives\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3482a4b5-e06c-4b4b-8706-803fd26ec1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "problem_file = \"/home/vmuser/Assignments/problem.ipynb\"\n",
    "solution_file = \"/home/vmuser/evaluation/solution.ipynb\"\n",
    "result_csv = \"/home/vmuser/evaluation/evaluation_results.csv\"\n",
    "result_json = \"/home/vmuser/evaluation/evaluation_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758b9a5b-dd73-4f74-9f9a-755b1f97b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task weightage (updated as per request)\n",
    "task_weightage = {\n",
    "    \"load_the_dataset\": 2,\n",
    "    \"process_store_data\": 1,\n",
    "    \"find_unique_values\": 1,\n",
    "    \"total_sales\": 1,\n",
    "    \"check_missing_values\": 1,\n",
    "    \"sales_distribution\": 1,\n",
    "    \"top_customer_segment\": 1,\n",
    "    \"regional_purchasing_behavior\": 1,\n",
    "    \"high_spending_regions\": 1,\n",
    "    \"popular_product_categories\": 1,\n",
    "    \"avg_quantity_per_transaction\": 1,\n",
    "    \"quantity_sales_relationship\": 1,\n",
    "    \"category_quantity_sales_trends\": 1,\n",
    "    \"highest_profit_segments\": 1,\n",
    "    \"clean_and_calculate_shipping_time\": 1,\n",
    "    \"calculate_discounted_price\": 1,\n",
    "    \"calculate_revenue_per_day\": 1,\n",
    "    \"identify_most_discounted_products\": 2,\n",
    "    \"analyze_revenue_efficiency\": 1,\n",
    "    \"plot_sales_distribution\": 1,\n",
    "    \"plot_sales_by_segmen\": 1,\n",
    "    \"plot_sales_trend\": 1,\n",
    "    \"plot_order_count_trend\": 1,\n",
    "    \"anova_sales_by_category\": 2,\n",
    "    \"ttest_sales_by_segment\": 2,\n",
    "    \"chi_square_category_region\": 2,\n",
    "    \"treat_outliers_iqr\": 1,\n",
    "    \"apply_outlier_treatment\": 1,\n",
    "    \"compute_correlations\": 1,\n",
    "    \"drop_var1\": 1,\n",
    "    \"normalize_numeric_columns\": 1,\n",
    "    \"one_hot_encode\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5314e658-95c0-4ead-90d7-0db021c87496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate attempt ID based on timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564620d0-ee95-40f8-b11b-c10fccd0d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_notebook(notebook_path):\n",
    "    \"\"\"Load a Jupyter Notebook and execute it as Python code.\"\"\"\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    exporter = PythonExporter()\n",
    "    source_code, _ = exporter.from_notebook_node(nb)\n",
    "    \n",
    "    global_namespace = {}\n",
    "    exec(\"import pandas as pd\\n\" + source_code, global_namespace)  # Ensure pandas is available\n",
    "    return global_namespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e102488d-aa0b-4f01-b504-9ad04998fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_function(user_globals, solution_globals, function_name):\n",
    "    \"\"\"Compare user and reference function outputs.\"\"\"\n",
    "    if function_name in user_globals and function_name in solution_globals:\n",
    "        user_func = user_globals[function_name]\n",
    "        solution_func = solution_globals[function_name]\n",
    "        \n",
    "        # Execute both functions\n",
    "        user_output = user_func()\n",
    "        solution_output = solution_func()\n",
    "        \n",
    "        return int(user_output == solution_output) * task_weightage.get(function_name, 1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63e63d13-0dfc-4e27-a2fa-a9d6f4e1822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_submission():\n",
    "    \"\"\"Evaluate user's submission against solution notebook.\"\"\"\n",
    "    user_globals = load_notebook(problem_file)\n",
    "    solution_globals = load_notebook(solution_file)\n",
    "    \n",
    "    scores = {}\n",
    "    total_score = 0\n",
    "    \n",
    "    for task in task_weightage.keys():\n",
    "        score = evaluate_function(user_globals, solution_globals, task)\n",
    "        scores[task] = score\n",
    "        total_score += score\n",
    "    \n",
    "    return scores, total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10e8f245-b74c-4cad-8030-7716d6b5898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(scores, total_score):\n",
    "    \"\"\"Save evaluation results to CSV and JSON.\"\"\"\n",
    "    attempt_id = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    result_entry = {\"AttemptID\": attempt_id, \"Timestamp\": timestamp, \"TotalScore\": total_score, **scores}\n",
    "    \n",
    "    # Append to CSV\n",
    "    df = pd.DataFrame([result_entry])\n",
    "    if os.path.exists(result_csv):\n",
    "        df.to_csv(result_csv, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(result_csv, mode='w', header=True, index=False)\n",
    "    \n",
    "    # Append to JSON\n",
    "    if os.path.exists(result_json):\n",
    "        with open(result_json, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        results = []\n",
    "    results.append(result_entry)\n",
    "    with open(result_json, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0383b356-088e-4ccf-b0f8-db70b4de93dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetailed Scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m, scores)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     scores, total_score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     save_results(scores, total_score)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Completed. Total Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_score)\n",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m, in \u001b[0;36mevaluate_submission\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_submission\u001b[39m():\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate user's submission against solution notebook.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     user_globals \u001b[38;5;241m=\u001b[39m \u001b[43mload_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     solution_globals \u001b[38;5;241m=\u001b[39m load_notebook(solution_file)\n\u001b[1;32m      6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mload_notebook\u001b[0;34m(notebook_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m source_code, _ \u001b[38;5;241m=\u001b[39m exporter\u001b[38;5;241m.\u001b[39mfrom_notebook_node(nb)\n\u001b[1;32m      9\u001b[0m global_namespace \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 10\u001b[0m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimport pandas as pd\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msource_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_namespace\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure pandas is available\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m global_namespace\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    scores, total_score = evaluate_submission()\n",
    "    save_results(scores, total_score)\n",
    "    print(\"Evaluation Completed. Total Score:\", total_score)\n",
    "    print(\"Detailed Scores:\", scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25f932-13af-4b91-a805-2741d1b3a61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
